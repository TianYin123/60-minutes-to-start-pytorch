{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9ed0d5-4edd-48f7-af5c-2f697c7d677d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1138, 0.8173, 0.0841, 0.6530, 0.2767],\n",
      "        [0.1576, 0.2298, 0.6188, 0.6300, 0.5020],\n",
      "        [0.3247, 0.7782, 0.9924, 0.0313, 0.8101]])\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Is the pytorch successfully installed?\n",
    "import torch\n",
    "import numpy as np\n",
    "a = torch.rand(3,5)\n",
    "print(a)\n",
    "b = torch.device('mps')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7502f0f-d7ca-4d19-9906-85de0d4c3f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is pytorch calculate on GPU?\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9c2828-3025-44b8-a0f6-382cf68c21c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n",
    "torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9ca0621-f5c8-4b7d-a0c0-75f6e4df66bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0.dev20230317'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f82bf-4a47-4701-a5bb-4cdbfafcad9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tensor Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e29325-3272-42e4-ac27-87fac742b957",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.Directly from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8b5de26-07f7-417f-aa10-99c3aede5635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors can be created directly from data. The data type is automatically inferred.\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e653e-80aa-43e6-a48e-85e119f9c945",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.From a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf9d47d-612d-47a0-acc1-8ddace1c8b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors can be created from NumPy arrays (and vice versa - see Bridge with NumPy https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label).\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1014b54-0238-427e-a186-1a00f0a231e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.From another tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f53ec37-4747-41cf-b242-c7eb74d17b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.9997, 0.6730],\n",
      "        [0.5209, 0.5577]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden（显式覆盖）.\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea8b971-d7b9-44e6-abab-5d42f7bec4cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.With random or constant values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a798bbc-4eaa-42fc-8e7d-4ece2df39111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.6214, 0.8485, 0.8936, 0.8008, 0.3649, 0.2769],\n",
      "        [0.3868, 0.5099, 0.7692, 0.8164, 0.6937, 0.0632]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "empty Tensor: \n",
      " tensor([[7.8675e+34, 4.6894e+27, 1.6217e-19, 7.3471e+28, 2.6383e+23, 2.7376e+20],\n",
      "        [6.3828e+28, 1.4603e-19, 1.8888e+31, 4.9656e+28, 7.9463e+08, 3.2604e-12]])\n"
     ]
    }
   ],
   "source": [
    "# shape is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n",
    "shape = (2, 6,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "empty_tensor = torch.empty(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n",
    "print(f\"empty Tensor: \\n {empty_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091ce57-53c3-4960-a282-cda01a84d22b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 总结  \n",
    "\n",
    "总结来说，一般 tensor_A 的初始化的方式有：  \n",
    "1.直接从已有数据 ex_data 获得：torch.tensor(ex_data)  \n",
    "2.由numpy向量np_array传得：torch.from_numpy(np_array)  \n",
    "3.由另一个 tensor_B 传得  \n",
    "\n",
    "    a.生成与 tensor_B 尺寸一致但内容不同的 tensor_A：torch.xx_like(tensor_B)  \n",
    "    b.通过设定规则(如\"dtype=torch.float\") 生成与 tensor_B 在规定处不同的 tensor_A：torch.xx_like(torch_B, dtype=torch.float)   \n",
    "4.根据已知shape生成  \n",
    "\n",
    "    a.随机生成：torch.rand(shape)  \n",
    "    b.全1生成：torch.ones(shape)  \n",
    "    c.全0生成：torch.zeros(shape)  \n",
    "    d.未初始化生成：torch.empty(shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f28c6d-8fa1-4e9f-8cc1-e00bdab82998",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tensor Attributes\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b0dc8b1-4f59-411d-9b43-57f073c9966c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee292ef-871e-43d4-897a-933ffa49471e",
   "metadata": {},
   "source": [
    "## 总结\n",
    "tensor有三个属性，分别是：  \n",
    "1.tensor的尺寸：tensor.shape  \n",
    "2.tensor的数据格式：tensor.dtype  \n",
    "3.tensor数据的存储位置：tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253e9bb-e4b0-4ebc-84b8-3aa789e172df",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  \"Bridge with numpy\"\n",
    "Data in tensor zand numpy share their underlying memory locations, and changing one will change another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3417c84d-3211-48c7-9713-336b1e6726cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# From tensor to numpy array\n",
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee77683d-a199-4bbd-853e-eff0c45a0a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc5457fe-caf0-4ed8-a014-2a0114554bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# From numpy array to tensor\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "# Changes in the NumPy array reflects in the tensor.\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90538b5-1a8b-4637-ab59-ff273a1ffaff",
   "metadata": {},
   "source": [
    "## 总结\n",
    "Tensor 和 Numpy 的数组可以相互转换，并且两者转换后共享在 CPU 下的内存空间，即改变其中一个的数值，另一个变量也会随之改变。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d10de0-e4ee-4e63-88c0-c03b4f8aeda7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tensor Operations\n",
    "Over 100 tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random sampling, and more are comprehensively described here(https://pytorch.org/docs/stable/torch.html).\n",
    "Each of them can be run on the GPU (at typically higher speeds than on a CPU). If you’re using Colab, allocate a GPU by going to Edit > Notebook Settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af22b6-9a89-4a48-9135-89ab8b02a12e",
   "metadata": {},
   "source": [
    "## Tensor with cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b3efc6a-aefe-4376-a59d-72e68f4df469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "  print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d455099-360e-4887-b7f3-d501f261308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当 CUDA 可用的时候，可用运行下方这段代码，采用 torch.device() 方法来改变 tensors 是否在 GPU 上进行计算操作\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # 定义一个 CUDA 设备对象\n",
    "    y = torch.ones_like(x, device=device)  # 显示创建在 GPU 上的一个 tensor\n",
    "    x = x.to(device)                       # 也可以采用 .to(\"cuda\") \n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # .to() 方法也可以改变数值类型\n",
    "# 输出结果\n",
    "# 第一个结果就是在 GPU 上的结果，打印变量的时候会带有 device='cuda:0'，而第二个是在 CPU 上的变量。\n",
    "# tensor([1.4549], device='cuda:0')\n",
    "# tensor([1.4549], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d8153e-ef5d-4d17-95e3-b066c777e77d",
   "metadata": {},
   "source": [
    "## 1.Standard numpy-like indexing and slicing:\n",
    "Try out some of the operations from the list. If you’re familiar with the NumPy API, you’ll find the Tensor API a breeze to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b457cb89-ef05-4b56-a9c0-08c57c5bfcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49447a1e-7bc6-4b7a-8053-77ca5fdf826e",
   "metadata": {},
   "source": [
    "## 2.Joining tensors\n",
    "Joining tensors You can use torch.cat to concatenate a sequence of tensors along a given dimension. See also torch.stack, another tensor joining op that is subtly different from torch.cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3123f0cd-93a2-4eda-a735-84863bdbbb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a01a8d-4e91-40eb-a7ac-70ddd992234d",
   "metadata": {},
   "source": [
    "## 3.Multiplying tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aa63fd3-7427-43c0-a4fc-a590897fba0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")\n",
    "# This computes the matrix multiplication between two tensors\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c714dd-bdad-434f-b447-c867a0babf90",
   "metadata": {},
   "source": [
    "## 4.In-place operations \n",
    "Operations that have a _ suffix are in-place. For example: x.copy_(y), x.t_(), will change x.  \n",
    "In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6b63028-9e6a-472e-8571-71a484311783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c63130-1475-43f3-a0df-67fca4a08030",
   "metadata": {},
   "source": [
    "## 总结\n",
    "1.对tensor的引索和切片可以按照numpy格式  \n",
    "2.tensor的拼接：torch.cat()\n",
    "3.tensor的四则运算：  \n",
    "    \n",
    "    a.加减法：直接使用'+'，'-'\n",
    "    b.乘法：\n",
    "    \n",
    "        i.tensor的叉乘：torch.mul(input_tensor,other) other可以是数字/tensor\n",
    "        ii.tensor的点乘：torch_A * torch_B\n",
    "        iii.tensor的宽泛矩阵乘积：tensor.matmul(tensor_1,tensor_2)  \n",
    "4.tensor_A的置换运算（即改变tensor_A本身的值）： \n",
    "\n",
    "    a.置换加：tensor_A.add_()\n",
    "    b.置换乘：tensor_A.mul_()\n",
    "    c.以此类推，基本上都有置换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a685b39-de41-4817-9256-3b3e2063321e",
   "metadata": {},
   "source": [
    "### 宽泛矩阵乘积的各种特殊情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4047bdd-b41e-4880-b491-252912e2b1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector x vector\n",
    "tensor1 = torch.randn(3)\n",
    "tensor2 = torch.randn(3)\n",
    "torch.matmul(tensor1, tensor2).size()\n",
    "# matrix x vector\n",
    "tensor1 = torch.randn(3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "torch.matmul(tensor1, tensor2).size()\n",
    "# batched matrix x broadcasted vector\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(4)\n",
    "torch.matmul(tensor1, tensor2).size()\n",
    "# batched matrix x batched matrix\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(10, 4, 5)\n",
    "torch.matmul(tensor1, tensor2).size()\n",
    "# batched matrix x broadcasted matrix\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(4, 5)\n",
    "torch.matmul(tensor1, tensor2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db5609a8-988b-4801-81ec-e682a6f3d1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1167,  0.6392, -1.2594])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d36ac2af-8a30-4d05-a10e-d08099216b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38215957-4373-4129-a8c7-24bc1876d011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
