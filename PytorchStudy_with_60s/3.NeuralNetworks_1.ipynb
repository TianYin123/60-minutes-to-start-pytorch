{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ea1c2b-6a33-464e-b75e-8ae489482718",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks can be constructed using the ``torch.nn`` package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9f666-9c57-4032-9ce9-773363e1dbbc",
   "metadata": {},
   "source": [
    "## 1.Neural Network Defination\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or\n",
    "  weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the network’s parameters\n",
    "- Update the weights of the network, typically using a simple update rule:\n",
    "  ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "For example, look at this network that classifies digit images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "637b0941-b990-4b80-877d-4f83c146bca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Code below is a 5 layers CNN,including 2 convolution layer and 3 fully connected layer.   \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc8c9a-0767-4cd7-810a-79f9756ed8c1",
   "metadata": {},
   "source": [
    "You just have to define the ``forward`` function, and the ``backward``\n",
    "function (where gradients are computed) is automatically defined for you\n",
    "using ``autograd``.\n",
    "You can use any of the Tensor operations in the ``forward`` function.\n",
    "\n",
    "The learnable parameters of a model are returned by ``net.parameters()``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f0b218-1b53-440d-a8fa-1c66cad931d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251f0e5-f8c0-41d2-a4af-5a179512b66d",
   "metadata": {},
   "source": [
    "Let's try a random 32x32 input. Note: expected input size of this net (LeNet) is 32x32. To use this net on the MNIST dataset, please resize the images from the dataset to 32x32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d232c89-33cf-4364-811e-8a5ef5913786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0421, -0.0726,  0.0196,  0.0210, -0.0853, -0.0247, -0.0119,  0.0458,\n",
      "         -0.0202,  0.1098]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185d7c0-aebb-4ecb-957a-1200e48bec07",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parameters and backprops with random\n",
    "gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6033d54a-5f35-4c26-9faa-31e954f82504",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d917247-4450-4310-ae95-ad882174b4f9",
   "metadata": {},
   "source": [
    "## 2.Loss Function\n",
    "A loss function takes the (output, target) pair of inputs, and computes a\n",
    "value that estimates how far away the output is from the target.  \n",
    "损失函数的输入是 (output, target) ，即网络输出和真实标签对的数据，然后返回一个数值表示网络输出和真实标签的差距。  \n",
    "PyTorch 中其实已经定义了不少的损失函数(PyTorch 中其实已经定义了不少的损失函数()，这里仅采用简单的均方误差：nn.MSELoss ，例子如下：)，这里仅采用简单的均方误差：nn.MSELoss ，例子如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd1b6f4-e757-49f8-8cbc-46d60039ebf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9592, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee317e3-6b4c-43eb-a0cd-ed15836ef77d",
   "metadata": {},
   "source": [
    "这里，整个网络的数据输入到输出经历的计算图如下所示，其实也就是数据从输入层到输出层，计算 loss 的过程。\n",
    "\n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d \n",
    "\n",
    "          -> view -> linear -> relu -> linear -> relu -> linear  \n",
    "      \n",
    "          -> MSELoss   \n",
    "      \n",
    "          -> loss \n",
    "如果调用 loss.backward() ，那么整个图都是可微分的，也就是说包括 loss ，图中的所有张量变量，只要其属性 requires_grad=True ，那么其梯度 .grad张量都会随着梯度一直累计。\n",
    "\n",
    "用代码来说明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d272afb-7ce2-45f1-bce6-4eebcc8b996b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x1113d6d90>\n",
      "<AddmmBackward0 object at 0x1113d63a0>\n",
      "<AccumulateGrad object at 0x1113d6d90>\n"
     ]
    }
   ],
   "source": [
    "# MSELoss\n",
    "print(loss.grad_fn)\n",
    "# Linear layer\n",
    "print(loss.grad_fn.next_functions[0][0])\n",
    "# Relu\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e4206-8208-430f-83ce-adab1c20ac31",
   "metadata": {},
   "source": [
    "## 3.Backprop\n",
    "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
    "You need to clear the existing gradients though, else gradients will be\n",
    "accumulated to existing gradients.\n",
    "\n",
    "\n",
    "Now we shall call ``loss.backward()``, and have a look at conv1's bias\n",
    "gradients before and after the backward.\n",
    "\n",
    "反向传播的实现只需要调用 ``loss.backward()`` 即可，当然首先需要清空当前梯度缓存，即``.zero_grad()`` 方法，否则之前的梯度会累加到当前的梯度，这样会影响权值参数的更新。\n",
    "\n",
    "下面是一个简单的例子，以 ``conv1`` 层的偏置参数 ``bias`` 在反向传播前后的结果为例：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e5b8a3f-62f4-44fa-9d90-2366be754cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0059,  0.0024, -0.0024, -0.0043,  0.0127, -0.0081])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feaa78f-636e-4203-b4ce-cacc623ad302",
   "metadata": {},
   "source": [
    "## 4.Update the weights\n",
    "The simplest update rule used in practice is the Stochastic Gradient\n",
    "Descent (SGD):  \n",
    "采用随机梯度下降(Stochastic Gradient Descent, SGD)方法的最简单的更新权重规则如下：\n",
    ".. code:: python\n",
    "\n",
    "    weight = weight - learning_rate * gradient\n",
    "\n",
    "We can implement this using simple Python code:  \n",
    "按照这个规则，代码实现如下所示：  \n",
    ".. code:: python\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "However, as you use neural networks, you want to use various different\n",
    "update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
    "To enable this, we built a small package: ``torch.optim`` that\n",
    "implements all these methods. Using it is very simple:  \n",
    "\n",
    "但是这只是最简单的规则，深度学习有很多的优化算法，不仅仅是 ``SGD``，还有 ``Nesterov-SGD, Adam, RMSProp`` 等等，为了采用这些不同的方法，这里采用 ``torch.optim`` 库，使用例子如下所示：  \n",
    "\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    import torch.optim as optim\n",
    "\n",
    "    # create your optimizer\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "    # in your training loop:\n",
    "    optimizer.zero_grad()   # zero the gradient buffers 清空缓存\n",
    "    output = net(input)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update 更新权重\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527cc198-a136-4d76-850d-eb9d367ca128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
